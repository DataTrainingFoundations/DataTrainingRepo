from pyspark.sql import SparkSession

spark:SparkSession = SparkSession.builder \
    .master("local")\
    .appName("spark_rdd_1")\
    .getOrCreate()

sc = spark.sparkContext



rdd1 = sc.parallelize(range(10))
print(rdd1.collect())


rdd2 = rdd1.map(lambda x: {x:x*10})

#now I have a dictionary
dict_list = rdd2.collect()
print(dict_list)
dict1={}
for row in dict_list:
    dict1.update(row)

print(dict1)

dict2 = {k: v for row in dict_list for k, v in row.items()}

print(dict2)

print(dict1[5])

#######################################
#Zip
a = sc.parallelize(["dog","almon"])
b= a.map(lambda x: len(x))

c=a.zip(b)

#Filter
a = sc.parallelize(range(1,10))
b=a.filter(lambda x:x%2 ==0)

b.collect()

#Group By (gives key value pairs)
a = sc.parallelize(range(1,10))
b=a.groupBy(lambda x:"even" if x%2 == 0 else "odd")
b.collect() 

#this looks a little neater but still missing the even and odd
print(list(b.collect()[0][1])) 

#instead, thank you Eric, we should use mapValues

bconverted = b.mapValues(list).collect()

print(bconverted)

##KeyBy / Join / ToDebugString
a = sc.parallelize(["hi","hello","world","are"])
#to make a key value pair just use keyBy
b = a.keyBy(lambda x: len(x))
print(b.collect())
c= sc.parallelize(["hi","hello","koi","two"])

d= c.keyBy(lambda x: len(x))

k=b.join(d)

print(k.toDebugString()) #to show lineage




spark.stop()